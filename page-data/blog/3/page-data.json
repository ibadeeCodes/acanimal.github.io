{"componentChunkName":"component---src-templates-blog-list-js","path":"/blog/3","webpackCompilationHash":"4776e6576de27e274acc","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"66ce5730-e55d-5864-9f36-be72f4229d10","frontmatter":{"title":"Closing 2015","date":"31 December, 2015","layout":"post"},"fields":{"slug":"/blog/2015/12/31/Closing-2015/","readingTime":{"text":"2 min read"}},"excerpt":"2015 is finishing. Its 18:15h while writing these lines. Looking back to this year lot of feelings mixes in my mind, some of them great and some sad. But live continues and we can only go forward. At professional level the most important fact was leaving Servei Meteorològic de Catalunya (meteo.cat) after nine years. I leave great workmates. We made some incredible things and, in the past year we work in some nice projects (a new website, a mobile ready web and an API to access weather data). Although probably the thing what I'm most proud with, as a team leader, was the fact to help creating a positive and constructive environment, always looking to learn and improve. Anyway, decisions are not easy to take and never likes to everybody. Well, I'm human like anyone, I can be wrong, but do not regret from anything I was done. You have enemies? Good. That means you’ve stood up for something, sometime in your life. - Winston Churchill A man with no enemies is a man with no character. - Paul Newman This year also start working at CartoDB, a young and really motivated company. I have been the last couple of months diving into code, learning and helping. I can't say much more, but the fact we are working on something really awesome for this new year :) At personal level this was a really hard year. The death of my father seems something a can't overcome. After a suddenly and aggressive cancer he passes away on April. The few months he lived after cancer was detected were a real hell for the family, and specially for mother. He always tried to have mental strength but seeing yourself deteriorating at giant steps is hard enough to make you fall and seeing someone you love dying a bit more every day, while trying to demonstrate strength, is something that is going accumulating in your mind. Simply writing these lines tears comes to my eyes. He teach me to have an open mind. He was the one that shows me what principles mean. I always thought if my father hadn't been my father we had been great friends. He always was proud about. See you some day friend ;) Live continues and we must continue going forward."}},{"node":{"id":"459c988f-d477-508a-93a4-d8a83a3c3812","frontmatter":{"title":"Reading very big JSON files in stream mode with GSON","date":"23 October, 2015","layout":"post"},"fields":{"slug":"/blog/2015/10/23/reading-json-file-in-stream-mode-with-gson/","readingTime":{"text":"5 min read"}},"excerpt":"JSON is everywhere, it is the new fashion file format (see you XML). JSON format is mainly used on REST APIs because it is easy to read by JavaScript  (JSON means JavaScript Object Notation) allowing to develop client side application. In Java and, similarly to the old days of XML, we have different ways to read JSON files: object model or streaming way. No one is better than other, they are simply designed for different needs and situations. The object model way works loading the whole file in memory and translating each JavaScript object to a Java object containing all the properties, array objects, etc. It is similar to the DOM way to read XML files. On the other hand, the stream way reads the file byte by byte and allows to apply actions when an object starts, an array ends, etc. It is similar to SAX way to read XML files. There is a third way to read a JSON file, similarly to StAX with XML, that combines the streaming way to read with the object model. For example, we can read a file in stream mode and when found an object start then read the whole object into memory. Which method is better depends on your needs. If your JSON is small the object model is great because you can load all the data and work as a normal Java objects, searching within an array, iterating, etc. When the file is really big you'll probably don't want to load it all into memory, so the streaming model is the best choice. The scene We are working on an API implemented in Java and one of the operations requires to open a big JSON file (about 10Mb) an returns an object identified by a given string. The file in question is formed by an array of objects, tons of object, and it has no sense to read the whole file and create tons of Java objects into memory only to return one. So, this is a good scenario to read the JSON file in stream mode. The code Before to continue it is worth to mention the code you will see here is available at java-json-examples repository. I have generated a 2.5MB JSON file using the mockjson tool. The data file is formed by an array ob person objects. Each person has an , , a  status and a list of  and : {% highlight json %}\n[\n  {\n    \"id\" : 0,\n    \"married\" : true,\n    \"name\" : \"George Moore\",\n    \"sons\" : null,\n    \"daughters\" : \n      {\n        \"age\" : 25,\n        \"name\" : \"Elizabeth\"\n      },\n      {\n        \"age\" : 28,\n        \"name\" : \"Nancy\"\n      },\n      {\n        \"age\" : 9,\n        \"name\" : \"Sandra\"\n      }\n    \n  },\n  ...\n]\n{% endhighlight %} GSON We are using GSON to read JSON files in Java. Take a look to the wikipedia link so you can see some of its main features. The other one great library to work with JSON data is Jackson. Both have similar features and performance. Our  model class When GSON reads JSON data it allows to unmarshal it to a given Java class. For this purpose we have created the  class which will store all the previous information (for the sample we are only stoting the person  and ): {% highlight java %}\npublic class Person {\n    private int id;\n    private String name; }\n{% endhighlight %} Reading using object model Next code shows how to read the JSON file using the object model. Remember, we are loading the whole file in memory converting data to  java class. {% highlight java %}\npublic static void readDom() {\n    BufferedReader reader = null;\n    try {\n        reader = new BufferedReader(new FileReader(file));\n        Gson gson = new GsonBuilder().create();\n        Person[] people = gson.fromJson(reader, Person[].class); }\n{% endhighlight %} The magic occurs within the line: . We are telling GSON to read the file and return an array of  objects. From here, if we want to return a given person we need to find it within the array. Let's take a look how we can do the same using the stream mode. Reading using the stream mode What we really are going to do here is to use a mixed mode between stream and object model. We are going to read file in stream mode and each time we found the start of a person object we are going to unmarshal it using the object model and we will repeat the process until we found the desired object. {% highlight java %}\npublic static void readStream() {\n    try {\n        JsonReader reader = new JsonReader(new InputStreamReader(stream, \"UTF-8\"));\n        Gson gson = new GsonBuilder().create(); }\n{% endhighlight %} With this approach we are loading all the objects to memory but one by one and not the whole file at once. Conclusion We have seen that JSON files, like XML, can be read with different techniques. Object model, stream mode or a mix of both. It is up to you to chose one depending on your needs. Object model allows to read a whole file and offers greater abstraction, because you can unmarshal data to your custom Java object model. Stream mode allows to read big files consuming fewer memory. As a cons, it implies a more low level reading but can be improved mixing both models (like in the example)."}},{"node":{"id":"fc0beb5e-2192-594f-9fb9-615f88e58ace","frontmatter":{"title":"Webpack, not another task runner tool","date":"15 October, 2015","layout":"post"},"fields":{"slug":"/blog/2015/10/15/webpack-not-another-task-runner-tool/","readingTime":{"text":"2 min read"}},"excerpt":"It seems we are living an gold era about JavaScript and front end world with a myriad of frameworks, language improvements and, what is related to this article, build systems, tasks runner or whatever you want to call them.\nWe have adepts to Grunt, firm believers of Gulp or purists preferring the use of old fashion npm scripts way. Well, I'm sorry for all of you but there is a new kid on the block and it is (IMO) a really strong competitor. It's name is webpack and it is a module bundler. OMG !!! A module what? What is webpack ? Webpack is a module bundler. It has nothing to do with a tasks runner, although in many cases can substitute the need of gulp or grunt. Webpack understands about modules and its dependencies (among JavaScript files, CSS or whatever) and generates assets. Probably you don't understand the importance of the last sentence, so I repeat it again: Remember webpack understands about modules and its dependencies and is good to generate assets from it. That is probably the main impact I see when developing with webpack. To get all its potential you need to change your mind from programming a set of JavaScript files, that finally and concatenated and minimized, to a set of modules, that exports variables and has dependencies among them. A brief presentation Here I present a short slideshow I prepared to introduce webpack to my team. Any feedback will be appreciated. You can also view at slid.es The two samples present in the slideshow are available at github at the webpack-presentation repository."}},{"node":{"id":"17aa8962-ef6e-517c-9467-5912117889fa","frontmatter":{"title":"Git, a short introduction with some pictures","date":"23 September, 2015","layout":"post"},"fields":{"slug":"/blog/2015/09/23/git-a-short-introduction-with-some-pictures/","readingTime":{"text":"1 min read"}},"excerpt":"This is a short introduction about Git version control system I made some time ago describing the main topics. Currently, Git is probably one of the most powerful and most used version control systems, so never is late to start with it. You can also view at slid.es"}},{"node":{"id":"604ed1b9-f1ca-53a5-b9c1-cda5afe4f898","frontmatter":{"title":"Specification pattern for NodeJS","date":"21 September, 2015","layout":"post"},"fields":{"slug":"/blog/2015/09/21/specification-pattern-for-nodejs/","readingTime":{"text":"4 min read"}},"excerpt":"Although the specification pattern is mainly use in DDD to check business rules, I think the idea of combine rules offers great flexibility in any application architecture, it is suitable for any kind of validations, simplifying and improving reusability and making code clearer. Because of this, some days ago I started working on an implementation of the specification pattern for NodeJS. The code is freely available at github repository and also installable via npmjs. The specification pattern There are tons of good documents and tutorials about the pattern, so I don't want to extent too much here. The best source of information, IMO, is the Eric Evans big blue book Domain-Driven Design: DDD The specification pattern is powerful enough to be used for: validations, queries and creation of objects that satisfies some criteria. So, take into account this post is only related to the first options: the validation of objects that satisfies some critaria. An specification is a piece of code that checks if a business rules is satisfied or not. For example, given a bug tracking system we can create two specification to check if a software project: has few issues if the number of issues reported last month is less 10, and is updated project if the date of the last solved issued it not beyond a week. The great of specifications is we can easily combine them to create complex rules reusing the code, for example, we can create the specification quality project that means a project has few issues and is an updated project. Wikipedia has a nice image showing an UMl diagram class about the specification patter: UML specification pattern We can see a specification is any interface that implements the  method and has the ,  and  method to chain specifications. How to use the NodeJS implementation ? I have created two implementations of the pattern: asynchronous and synchronous versions. The synchronous version is fine for those in-memory validations, for example when you do not require query a database. The asynchronous version, on the other hand, is suitable for those cases in which the validation depends on an asynchronous source, like a file, a query to an API, etc. The first step to use the implementation is to include the required version (synchronous or asynchronous): For each business rule (or validation) you need to check, a specification must be created. Next code creates a specification that checks if a number is greater than the one indicated at the specification: Later, to use the previous specification: The base class  offers the ,  and  methods we can use to chain specifications and build complex ones. For example: The asynchronous version is suitable if you need to check agains an asynchronous source, like a database, files, etc. The only difference is the way to implement the  method, which must be use a callback, for example: and to use it you can make via the callback: Chaining specifications works in the same way as the synchronous version, simply remember the only difference is the way to use the  method: Conclusions The post presents a dual implementation for NodeJS, synchronous and asynchronous. There is no reason to use design patterns in a multi-paradigm language like JavaScript. Specification pattern can help when working with validations, simplifying reusability and allowing validations chaining through specifications."}},{"node":{"id":"9fd50913-90e6-5b23-8d72-375e323f81d3","frontmatter":{"title":"I'm working on ClydeIO","date":"14 September, 2015","layout":"post"},"fields":{"slug":"/blog/2015/09/14/i-m-working-on-clydeio/","readingTime":{"text":"5 min read"}},"excerpt":"ClydeIO is an open source API gateway and management layer based on nodejs. It is extremely flexibly, configurable and simply to extend. It is designed to simplify the development of new APIs (or simply improving existing ones) adding authentication, logging, rate limiting, etc. Note, the project is currently a proof of concept (I'm currently working to apply it in a real system) and most of the developed filters are basic implementation to demonstrate its usefulness. What is an API gateway and why I need one? The most important part when designing and implementing an API is to model your business correctly and offer a set of well defined operations accordingly with them. Unfortunately, that is only an small part of the job and it is not enough to ensure the success. As a real World system you require to secure your API, store logs, apply rate limits, etc. The task of publishing an API becomes much more complex than understanding your business, you have entered the world of security, monitoring and... the unknown !!! An API gateway is a single point of entry responsible to apply actions (like security or logging) before redirecting the request to your real-do-the-job API. We can see the gateway as a kind (or mix) of firewall and proxy and is really useful implementing microservices.  Thanks to ClydeIO you can spend your efforts implementing your business API leaving the rest of things to the gateway. The glory of ClydeIO is its simplicity and its easy to extent with new filters like: Secure an existent API Log access to any resource Rate limiting Filter request (allow/deny) depending on the query parameters Cache data or whatever you need  Because it is based on node we can use the myriad of awesome node packages out there and integrate within your filters.  Why a new system instead contribute to an existent one? I was looking for similar projects before start ClydeIO. There is plenty of services that provides same functionalities (and many more) as well as many projects with a great maturity level, but no one satisfies my needs. Services implies a cost for its usage that, sometimes, can be hard to calculate and in some cases requires you adapt your systems (your business API) to accommodate to the service requirements. Other software projects means you must be comfortable with the technology they are implemented with, mainly its programming language and database used to store configuration and information. One thing I saw in common in most of the software projects is the fact they do what they do, that is, they are prepared to make many things, do it well but are not allowed to extend the gateway easily with new requirements users can have. They are designed to make the most common things: rate limiting, security, logging, etc but it is hard to know how we can extend the gateway to send us an email when there was more than ten invalid accesses. In addition, I found some of them really complex to configure, based on monster XML configuration files. Once last comment on why I created ClydeIO: to take advantage of the node modules. There exists other API gateways implemented using NGINX server, lua, go or python language but nonetheless implemented with node. To be honest I must point here the StrongLoop LoopBack API Gateway product. Current status ClydeIO is currently a proof of concept and I have implemented a bunch of filters to test its capabilities. Currently all configuration is provided via a JSON file. That's nice and simply but not much secure when working with authentication filters that needs specify users and passwords or with a real scenario that requires manage hundred of users. Because of this I'm currently working hard trying to create the configuration module, responsible to manage the whole configuration, and designing to be easy to implement for different backends: memory, redis, mongodb, postgresql, ... I have great feeling about ClydeIO's possibilities but to be honest it is currently a personal side project I write on my few free time. I have no contributors neither sponsors. So, if you arrive to this page and are interested in the project feel free to contact with me and start helping with your money or time :) Documentation I have create the ClydeIO github organization to host all the related projects related with ClydeIO. We can differentiate among the core project, so called clydeio too and the rest of projects that are clyde's filters. The current core project documentation can be found at the project's wiki: https://github.com/clydeio/clydeio/wiki. It will probably change soon, once finished the configuration module, but the concepts remains the same. Contributions As a said, for the moment this is a personal project I develop on my free time. So don't hesitate to contact with me for any kind of support and help."}}]}},"pageContext":{"isCreatedByStatefulCreatePages":false,"limit":6,"skip":12,"numPages":18,"currentPage":3}}}