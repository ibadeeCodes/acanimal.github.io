{"componentChunkName":"component---src-templates-blog-list-js","path":"/blog/3","webpackCompilationHash":"94830e9bd36aba932e6a","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"d290be60-47df-5c9d-807c-485640183f23","frontmatter":{"title":"Symfony, images and S3","date":"25 March, 2016","layout":"post"},"fields":{"slug":"/blog/2016/03/26/symfony-and-s3/","readingTime":{"text":"6 min read"}},"excerpt":"Paraphrasing the movie title Sex, lies and videotape, this post is related on how I configured my symfony project to work with images (including thumbnail generation) and store all them on Amazon S3 service. There are are libraries and bundles to work with images and also others to work with S3, but the combination of all them can be a tricky experience with not much documentation out there. In addition, there is also one more challenge to achieve and, that is, while developing I want to store images locally (in a directory) but when deployed I want to use S3 storage. Currently I'm working on a project where users can upload photos. The application allows users to create collections, drag and drop images, order them and finally upload to the server. In addition, the user can explore his/her collections, browse images and download collections in a single ZIP file. All this having in mind: While developing locally we want to store images in the local server folder. In staging or production environment we want to use S3. Original images must remain private, while the thumbnails must be public to be included in the application web pages. We need to generate thumbnails with different sizes. When a user access to a collection a list of small thumbnails are shown. When user clicks an image a medium thumbnail is presented. When user downloads a collection the ZIP file must include the original images. So, in summary, we need to deal with image upload, thumbnail generation and S3 service. Uploading images For image uploading we have used the VichUploaderBundle. Uploading images isn't a secret but can involve some extra tasks. The VichUploaderBundle helps working with file uploads that are attached to ORM entities, that is, it is responsible to move the images to some configured place and attach it to your entity. In addition, we want to store images in folders classified by user and collection, something like , where  and  are identifiers. A nice feature VichUploaderBundle offers is the possibility to attach the so called directory namer or file namer that determines the final name for the upload file. This way when a file is uploaded, given the current user and the selected collection, we determine dynamically the target folder where the bundle must store the image. Note the ORM entity only has the image file name. The path to the file is computed through the specified directory and/or file namers. For this reason, the bundle also includes the methods require to get the absolute path to a file given the file name stored within the entity. Generating thumbnails To generate thumbnails we have used the LiipImagineBundle bundle. With it, when you reference an image within your templates you don't get the original image but a new one obtained applying some filters. Next line shows how to include an image in your twig template obtained after applying a  configuration: The good thing is LiipImagineBundle generates the thumbnails when images are first time accessed and stores them in a cache folder for next calls. Abstracting the file system The issue is we want to upload images and generate thumbnails into a local folder at development time and to S3 when running in staging or production. Hopefully for us there is the Gaufrette bundle, which is an abstract filesystem. It offers a common interface to read/write to different filesystem and a bunch of implementations to work against the local filesystem, an FTP server, Amazon S3 service, ... and many more. Putting it all together Arrived here, the main question is how to configure the three bundles to work together, in summary: We want to abstract the filesystem to work locally while developing and with S3 in production. We need to upload images. We need to generate thumbnails for uploaded images and store them in a cache folder to be later server. We have divided the configuration between the  file and the . The first contains the configuration for the previous three bundles ready to work locally. The second overrides some propertires to work in production, using S3. The first point is to configure the Gaufrette bundle to abstract our filesystem. Next is the configuration in the : Compare with parameters we override in the . Note for production you need to define an AWS-S3 service which I do not include here. We define a  filesystem which by default uses a  adapter and in production uses an  one. Next step is to configure the VichUploaderBundle bundle. Hopefully it is designed to integrate with Gaufrette so it is easy to specify how to upload files through gaufrette. Next is the configuration: As you can see we are specifying we want to use gaufrette with  and the upload destination is the previous defined gaufrette filesystem . This means all images will be uploaded through the Gaufrette filesystem to that destination. Note, within the target filesystem, the final folder and file name are determined by a custom directory namer we have implemented ( which adds the user ID to the path) and the file namer  offered by Gaufrette, which assigns a unique name to each file. Finally, we need to configure the LiipImagineBundle bundle. Next is the configuration used for local development. We need to specify the cache folder where to generate the thumbnails in adition to our filter, that will generate with size  and half quality: Main properties to configure are the and the . The first one uses the stream  that uses gaufrette filesystem. The second uses the resolver  that we have configured to use the local folder . For production, configuration changes slightly. Here we override the resolver to generate cache files through the resolver  which points to S3 bucket: Conclusions VichUploaderBundle, LiipImagineBundle and Gaufrette are three great Symfony2 bundles. The configuration to make work all them can by tricky so hope this post can help others. While VichUploaderBundle is designed to work with Gaufrette, and its configuration is almost trivial, LiipImagineBundle is not and requires some extra tasks. For LiipImagineBundle we need to configure its main components, which are the cache and the ."}},{"node":{"id":"66ce5730-e55d-5864-9f36-be72f4229d10","frontmatter":{"title":"Closing 2015","date":"31 December, 2015","layout":"post"},"fields":{"slug":"/blog/2015/12/31/Closing-2015/","readingTime":{"text":"2 min read"}},"excerpt":"2015 is finishing. Its 18:15h while writing these lines. Looking back to this year lot of feelings mixes in my mind, some of them great and some sad. But live continues and we can only go forward. At professional level the most important fact was leaving Servei Meteorològic de Catalunya (meteo.cat) after nine years. I leave great workmates. We made some incredible things and, in the past year we work in some nice projects (a new website, a mobile ready web and an API to access weather data). Although probably the thing what I'm most proud with, as a team leader, was the fact to help creating a positive and constructive environment, always looking to learn and improve. Anyway, decisions are not easy to take and never likes to everybody. Well, I'm human like anyone, I can be wrong, but do not regret from anything I was done. You have enemies? Good. That means you’ve stood up for something, sometime in your life. - Winston Churchill A man with no enemies is a man with no character. - Paul Newman This year also start working at CartoDB, a young and really motivated company. I have been the last couple of months diving into code, learning and helping. I can't say much more, but the fact we are working on something really awesome for this new year :) At personal level this was a really hard year. The death of my father seems something a can't overcome. After a suddenly and aggressive cancer he passes away on April. The few months he lived after cancer was detected were a real hell for the family, and specially for mother. He always tried to have mental strength but seeing yourself deteriorating at giant steps is hard enough to make you fall and seeing someone you love dying a bit more every day, while trying to demonstrate strength, is something that is going accumulating in your mind. Simply writing these lines tears comes to my eyes. He teach me to have an open mind. He was the one that shows me what principles mean. I always thought if my father hadn't been my father we had been great friends. He always was proud about. See you some day friend ;) Live continues and we must continue going forward."}},{"node":{"id":"459c988f-d477-508a-93a4-d8a83a3c3812","frontmatter":{"title":"Reading very big JSON files in stream mode with GSON","date":"23 October, 2015","layout":"post"},"fields":{"slug":"/blog/2015/10/23/reading-json-file-in-stream-mode-with-gson/","readingTime":{"text":"5 min read"}},"excerpt":"JSON is everywhere, it is the new fashion file format (see you XML). JSON format is mainly used on REST APIs because it is easy to read by JavaScript  (JSON means JavaScript Object Notation) allowing to develop client side application. In Java and, similarly to the old days of XML, we have different ways to read JSON files: object model or streaming way. No one is better than other, they are simply designed for different needs and situations. The object model way works loading the whole file in memory and translating each JavaScript object to a Java object containing all the properties, array objects, etc. It is similar to the DOM way to read XML files. On the other hand, the stream way reads the file byte by byte and allows to apply actions when an object starts, an array ends, etc. It is similar to SAX way to read XML files. There is a third way to read a JSON file, similarly to StAX with XML, that combines the streaming way to read with the object model. For example, we can read a file in stream mode and when found an object start then read the whole object into memory. Which method is better depends on your needs. If your JSON is small the object model is great because you can load all the data and work as a normal Java objects, searching within an array, iterating, etc. When the file is really big you'll probably don't want to load it all into memory, so the streaming model is the best choice. The scene We are working on an API implemented in Java and one of the operations requires to open a big JSON file (about 10Mb) an returns an object identified by a given string. The file in question is formed by an array of objects, tons of object, and it has no sense to read the whole file and create tons of Java objects into memory only to return one. So, this is a good scenario to read the JSON file in stream mode. The code Before to continue it is worth to mention the code you will see here is available at java-json-examples repository. I have generated a 2.5MB JSON file using the mockjson tool. The data file is formed by an array ob person objects. Each person has an , , a  status and a list of  and : {% highlight json %}\n[\n  {\n    \"id\" : 0,\n    \"married\" : true,\n    \"name\" : \"George Moore\",\n    \"sons\" : null,\n    \"daughters\" : \n      {\n        \"age\" : 25,\n        \"name\" : \"Elizabeth\"\n      },\n      {\n        \"age\" : 28,\n        \"name\" : \"Nancy\"\n      },\n      {\n        \"age\" : 9,\n        \"name\" : \"Sandra\"\n      }\n    \n  },\n  ...\n]\n{% endhighlight %} GSON We are using GSON to read JSON files in Java. Take a look to the wikipedia link so you can see some of its main features. The other one great library to work with JSON data is Jackson. Both have similar features and performance. Our  model class When GSON reads JSON data it allows to unmarshal it to a given Java class. For this purpose we have created the  class which will store all the previous information (for the sample we are only stoting the person  and ): {% highlight java %}\npublic class Person {\n    private int id;\n    private String name; }\n{% endhighlight %} Reading using object model Next code shows how to read the JSON file using the object model. Remember, we are loading the whole file in memory converting data to  java class. {% highlight java %}\npublic static void readDom() {\n    BufferedReader reader = null;\n    try {\n        reader = new BufferedReader(new FileReader(file));\n        Gson gson = new GsonBuilder().create();\n        Person[] people = gson.fromJson(reader, Person[].class); }\n{% endhighlight %} The magic occurs within the line: . We are telling GSON to read the file and return an array of  objects. From here, if we want to return a given person we need to find it within the array. Let's take a look how we can do the same using the stream mode. Reading using the stream mode What we really are going to do here is to use a mixed mode between stream and object model. We are going to read file in stream mode and each time we found the start of a person object we are going to unmarshal it using the object model and we will repeat the process until we found the desired object. {% highlight java %}\npublic static void readStream() {\n    try {\n        JsonReader reader = new JsonReader(new InputStreamReader(stream, \"UTF-8\"));\n        Gson gson = new GsonBuilder().create(); }\n{% endhighlight %} With this approach we are loading all the objects to memory but one by one and not the whole file at once. Conclusion We have seen that JSON files, like XML, can be read with different techniques. Object model, stream mode or a mix of both. It is up to you to chose one depending on your needs. Object model allows to read a whole file and offers greater abstraction, because you can unmarshal data to your custom Java object model. Stream mode allows to read big files consuming fewer memory. As a cons, it implies a more low level reading but can be improved mixing both models (like in the example)."}},{"node":{"id":"fc0beb5e-2192-594f-9fb9-615f88e58ace","frontmatter":{"title":"Webpack, not another task runner tool","date":"15 October, 2015","layout":"post"},"fields":{"slug":"/blog/2015/10/15/webpack-not-another-task-runner-tool/","readingTime":{"text":"2 min read"}},"excerpt":"It seems we are living an gold era about JavaScript and front end world with a myriad of frameworks, language improvements and, what is related to this article, build systems, tasks runner or whatever you want to call them.\nWe have adepts to Grunt, firm believers of Gulp or purists preferring the use of old fashion npm scripts way. Well, I'm sorry for all of you but there is a new kid on the block and it is (IMO) a really strong competitor. It's name is webpack and it is a module bundler. OMG !!! A module what? What is webpack ? Webpack is a module bundler. It has nothing to do with a tasks runner, although in many cases can substitute the need of gulp or grunt. Webpack understands about modules and its dependencies (among JavaScript files, CSS or whatever) and generates assets. Probably you don't understand the importance of the last sentence, so I repeat it again: Remember webpack understands about modules and its dependencies and is good to generate assets from it. That is probably the main impact I see when developing with webpack. To get all its potential you need to change your mind from programming a set of JavaScript files, that finally and concatenated and minimized, to a set of modules, that exports variables and has dependencies among them. A brief presentation Here I present a short slideshow I prepared to introduce webpack to my team. Any feedback will be appreciated. You can also view at slid.es The two samples present in the slideshow are available at github at the webpack-presentation repository."}},{"node":{"id":"17aa8962-ef6e-517c-9467-5912117889fa","frontmatter":{"title":"Git, a short introduction with some pictures","date":"23 September, 2015","layout":"post"},"fields":{"slug":"/blog/2015/09/23/git-a-short-introduction-with-some-pictures/","readingTime":{"text":"1 min read"}},"excerpt":"This is a short introduction about Git version control system I made some time ago describing the main topics. Currently, Git is probably one of the most powerful and most used version control systems, so never is late to start with it. You can also view at slid.es"}},{"node":{"id":"604ed1b9-f1ca-53a5-b9c1-cda5afe4f898","frontmatter":{"title":"Specification pattern for NodeJS","date":"21 September, 2015","layout":"post"},"fields":{"slug":"/blog/2015/09/21/specification-pattern-for-nodejs/","readingTime":{"text":"4 min read"}},"excerpt":"Although the specification pattern is mainly use in DDD to check business rules, I think the idea of combine rules offers great flexibility in any application architecture, it is suitable for any kind of validations, simplifying and improving reusability and making code clearer. Because of this, some days ago I started working on an implementation of the specification pattern for NodeJS. The code is freely available at github repository and also installable via npmjs. The specification pattern There are tons of good documents and tutorials about the pattern, so I don't want to extent too much here. The best source of information, IMO, is the Eric Evans big blue book Domain-Driven Design: DDD The specification pattern is powerful enough to be used for: validations, queries and creation of objects that satisfies some criteria. So, take into account this post is only related to the first options: the validation of objects that satisfies some critaria. An specification is a piece of code that checks if a business rules is satisfied or not. For example, given a bug tracking system we can create two specification to check if a software project: has few issues if the number of issues reported last month is less 10, and is updated project if the date of the last solved issued it not beyond a week. The great of specifications is we can easily combine them to create complex rules reusing the code, for example, we can create the specification quality project that means a project has few issues and is an updated project. Wikipedia has a nice image showing an UMl diagram class about the specification patter: UML specification pattern We can see a specification is any interface that implements the  method and has the ,  and  method to chain specifications. How to use the NodeJS implementation ? I have created two implementations of the pattern: asynchronous and synchronous versions. The synchronous version is fine for those in-memory validations, for example when you do not require query a database. The asynchronous version, on the other hand, is suitable for those cases in which the validation depends on an asynchronous source, like a file, a query to an API, etc. The first step to use the implementation is to include the required version (synchronous or asynchronous): For each business rule (or validation) you need to check, a specification must be created. Next code creates a specification that checks if a number is greater than the one indicated at the specification: Later, to use the previous specification: The base class  offers the ,  and  methods we can use to chain specifications and build complex ones. For example: The asynchronous version is suitable if you need to check agains an asynchronous source, like a database, files, etc. The only difference is the way to implement the  method, which must be use a callback, for example: and to use it you can make via the callback: Chaining specifications works in the same way as the synchronous version, simply remember the only difference is the way to use the  method: Conclusions The post presents a dual implementation for NodeJS, synchronous and asynchronous. There is no reason to use design patterns in a multi-paradigm language like JavaScript. Specification pattern can help when working with validations, simplifying reusability and allowing validations chaining through specifications."}}]}},"pageContext":{"isCreatedByStatefulCreatePages":false,"limit":6,"skip":12,"numPages":18,"currentPage":3}}}